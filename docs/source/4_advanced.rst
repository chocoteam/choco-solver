**************Advanced usage **************.. _41_LNS_label:Large Neighborhood Search (LNS)===============================Local search techniques are very effective to solve hard optimization problems.Most of them are, by nature, incomplete.In the context of constraint programming (CP) for optimization problems, one of the most well-known and widely used local search techniques is the Large Neighborhood Search (LNS) algorithm [#q1]_.The basic idea is to iteratively relax a part of the problem, then to use constraint programming to evaluate and bound the new solution... [#q1] Paul Shaw. Using constraint programming and local search methods to solve vehicle routing problems. In Michael Maher and Jean-Francois Puget, editors, *Principles and Practice of Constraint Programming, CP98*, volume 1520 of *Lecture Notes in Computer Science*, pages 417â€“431. Springer Berlin Heidelberg, 1998.Principle---------LNS is a two-phase algorithm which partially relaxes a given solution and repairs it.Given a solution as input, the relaxation phase builds a partial solution (or neighborhood) by choosing a set of variables to reset to their initial domain;The remaining ones are assigned to their value in the solution.This phase is directly inspired from the classical Local Search techniques.Even though there are various ways to repair the partial solution, we focus on the technique in which Constraint Programming is used to bound the objective variable andto assign a value to variables not yet instantiated.These two phases are repeated until the search stops (optimality proven or limit reached).The ``LNSFactory`` provides pre-defined configurations.Here is the way to declare LNS to solve a problem: ::    LNSFactory.rlns(solver, ivars, 30, 20140909L, new FailCounter(100));    solver.findOptimalSolution(ResolutionPolicy.MINIMIZE, objective);It declares a *random* LNS which, on a solution, computes a partial solution based on ``ivars``.If no solution are found within 100 fails (``FailCounter(100)``), a restart is forced.Then, every ``30`` calls to this neighborhood, the number of fixed is randomly picked.``20140909L`` is the seed for the ``java.util.Random``.The instruction ``LNSFactory.rlns(solver, vars, level, seed, frcounter)`` runs:.. literalinclude:: /../../choco-solver/src/main/java/solver/search/loop/lns/LNSFactory.java   :language: java   :lines: 112-114   :linenos:The factory provides other LNS configurations together with built-in neighbors.Neighbors---------While the implementation of LNS is straightforward, the main difficulty lies in the design of neighborhoods able to move the search further.Indeed, the balance between diversification (i.e., evaluating unexplored sub-tree) and intensification (i.e., exploring them exhaustively) should be well-distributed.Generic neighbors^^^^^^^^^^^^^^^^^One drawback of LNS is that the relaxation process is quite often problem dependent.Some works have been dedicated to the selection of variables to relax through general concept not related to the class of the problem treated [5,24].However, in conjunction with CP, only one generic approach, namely Propagation-Guided LNS [24], has been shown to be very competitive with dedicated ones on a variation of the Car Sequencing Problem.Nevertheless, such generic approaches have been evaluated on a single class of problem and need to be thoroughly parametrized at the instance level, which may be a tedious task to do.It must, in a way, automatically detect the problem structure in order to be efficient.Combining neighborhoods^^^^^^^^^^^^^^^^^^^^^^^There are two ways to combine neighbors.Sequential""""""""""Declare an instance of ``SequenceNeighborhood(n1, n2, ..., nm)``.Each neighbor ni is applied in a sequence until one of them leads to a solution.At step k, the :math:`(k \mod m)^{th}` neighbor is selected.The sequence stops if at least one of the neighbor is complete.Adaptive""""""""Declare an instance of ``AdaptiveNeighborhood(1L, n1, n2, ..., nm)``.At the beginning a weight of 1 at assigned to each neighbor ni.Then, if a neighbor leads to solution, its weight :math:`w_i` is increased by 1.Any time a partial solution has to be computed, a value ``W`` between 1 and :math:`w_1+w_2+...+w_n` is randomly picked (``1L`` is the seed).Then the weight of each neighbor is subtracted from ``W``, as soon as ``W``:math:`\leq 0`, the corresponding neighbor is selected.For instance, let's consider three neighbors n1, n2 and n3, their respective weights w1=2, w2=4, w3=1.``W`` = 3  is randomly picked between 1 and 7.Then, the weight of n1 is subtracted, ``W``2-=1; the weight of n2 is subtracted, ``W``-4 = -3, ``W`` is less than 0 and n2 is selected.Defining its own neighborhoods^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^One can define its own neighbor by extending the abstract class ``ANeighbor``.It forces to implements the following methods:+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+| **Method**                                                             |   **Definition**                                                                                                       |+========================================================================+========================================================================================================================++------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+| ``void recordSolution()``                                              | Action to perform on a solution (typicallu, storing the current variables' value).                                     |+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------++------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+| ``void fixSomeVariables(ICause cause) throws ContradictionException``  | Fix some variables to their value in the last solution, computing a partial solution.                                  |+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------++------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+| ``void restrictLess()``                                                | Relax the number of variables fixed. Called when no solution was found during a LNS run (trapped into a local optimum).|+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------++------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+| ``boolean isSearchComplete()``                                         | Indicates whether the neighbor is complete, that is, can end.                                                          |+------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+Restarts--------A generic and common way to reinforce diversification of LNS is to introduce restart during the search process.This technique has proven to be very flexible and to be easily integrated within standard backtracking procedures [#q2]_... [#q2] Laurent Perron. Fast restart policies and large neighborhood search. In Francesca Rossi, editor, *Principles and Practice of Constraint Programming at CP 2003*, volume 2833 of *Lecture Notes in Computer Science*. Springer Berlin Heidelberg, 2003.Walking-------A complementary technique that appear to be efficient in practice is named `Walking` and consists in accepting equivalent intermediate solutions in a search iteration instead of requiring a strictly better one.This can be achieved by defining an ``ObjectiveManager`` like this: ::    solver.set(new ObjectiveManager(objective, ResolutionPolicy.MAXIMIZE, false));Where the last parameter, named ``strict`` must be set to false to accept equivalent intermediate solutions... _43_explanations_label:Explanations============Choco |version| natively support explanations [#1]_. However, no explanation engine is plugged-in by default... [#1] Narendra Jussien. The versatility of using explanations within constraint programming. Technical Report 03-04-INFO, 2003.Principle---------Nogoods and explanations have long been used in various paradigms for improving search.An explanation records some sufficient information to justify an inference made by the solver (domain reduction, contradiction, etc.).It is made of a subset of the original propagators of the problem and a subset of decisions applied during search.Explanations represent the logical chain of inferences made by the solver during propagation in an efficient and usable manner.In a way, they provide some kind of a trace of the behavior of the solver as any operation needs to be explained.Explanations have been successfully used for improving constraint programming search process.Both complete (as the mac-dbt algorithm) and incomplete (as the decision-repair algorithm) techniques have been proposed.Those techniques follow a similar pattern: learning from failures by recording each domain modification with its associated explanation (provided by the solver) and taking advantage of the information gathered to be able to react upon failure by directly pointing to relevant decisions to be undone.Complete techniques follow a most-recent based pattern while incomplete technique design heuristics to be used to focus on decisions more prone to allow a fast recovery upon failure.Key components of an explanation system^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Adding explanations capabilities to a constraint solver requires addressing several aspects:Computing explanations:    domain reductions are usually associated with a cause: the propagator that actually performed the modification.    This information can be used to compute an explanation.    This can be done synchronously during propagation (by intrusive modification of the propagation algorithm) or asynchronously post propagation (by accessing an explanation service provided by propagators).Storing explanations:    a data structure needs to be defined to be able to store decisions made by the solver, domain reductions and their associated explanations.    There exist several ways for storing explanations: a flattened storage of the domain modifications and their explanations composed of propagators and previously made decisions, or a un-flattened storage of the domain modifications and their explanations expressed through previous domain modifications.    The data structure is referred to as explanation store.Accessing explanations:    the data structure used to store explanations needs to provide access not only to domain modification explanations but also to current upper and lower bounds of the domains, current domain as a whole, etc.Despite being possibly very efficient, explanations suffer from several drawbacks:Memory:    storing explanations requires storing a way or another, variable modifications;CPU:    computing explanations usually comes with a cost even though the propagation algorithm can be partially used for that;Software engineering:    implementing explanations can be quite intrusive within a constraint solver.In practice-----------Consider the following example:.. literalinclude:: /../../choco-samples/src/test/java/docs/ExplanationExamples.java   :language: java   :lines: 50-54,57   :linenos:The problem has no solution since the two constraints cannot be satisfied together.A naive strategy such as ``ISF.lexico_LB(bvars)`` (which selects the variables in lexicographical order) will detect lately and many times the failure.By plugging-in an explanation engine, on each failure, the reasons of the conflict will be explained... literalinclude:: /../../choco-samples/src/test/java/docs/ExplanationExamples.java   :language: java   :lines: 55   :linenos:The explanation engine records *deductions* and *causes* in order to compute explanations.In that small example, when an explanation engine is plugged-in, the two first failures will enable to conclude that the problem has no solution.Only three nodes are created to close the search, seven are required without explanations.Deduction^^^^^^^^^There are five types of deductions: value removal, decision application, decision refutation, propagator activation and explanation.Value removal and branching decision    They are specific deductions; they store the touched couple variable-value. They are triggered by, respectively, propagation and search process.    The relation expressed can be value deletion or assignment.Propagator activation    It stores the touched propagator; it is mainly triggered through reification.Explanation    A specific deduction made of a set of deductions and a set of propagators.Cause^^^^^A cause implements ``ICause`` and must defined a ``explain(Deduction d, Explanation e)`` method.Such a method completes the explanation ``e`` with the help of the deduction ``d``.Every time a variable is modified, the cause needs to be specified in order to compute explanations.For instance, when a propagator updates the bound of an integer variable, the cause is the propagator itself.So do decisions, objective manager, etc.Computing explanations^^^^^^^^^^^^^^^^^^^^^^When a contradiction occurs during propagation, it can only be thrown by:- a propagator which detects unsatisfiability, based on the current domain of its variables;- or a variable whom domain is emptied.Consequently, in addition to causes, variables can also explain the current state of their domain.And, computing an explanation of a failure consists in recursive calls to ``explain`` methods from causes and variables.The entry point is either a the unsatisfiabable propagator or the empty variable... note::    Explanations can be computed without failure. The entry point is a variable, and only removed values can be explained.Variables explain themselves by iterating over removed values and calling the explanation engine to explain each of them.Each propagator embeds its own explanation algorithm which relies on the relation it defines over variables.By default, if no explanation algorithm is defined, a default algorithm is applied:the current deduction is due to the current domain of the involved variables and the application of the propagator.But, this is a weak explanation and providing specific explanations betters the overall process.For instance, here is the algorithm of ``PropGreaterOrEqualX_YC`` (:math:`x \geq y + c`, ``x`` and ``y`` are integer variables, ``c`` is a constant):.. literalinclude:: /../../choco-solver/src/main/java/solver/constraints/binary/PropGreaterOrEqualX_YC.java   :language: java   :lines: 114-116,118-119,121-127   :linenos:The two first lines indicate that the deduction is due to the application of the propagator (l.3), maybe through reification (l.2).Then, depending on the variable touched by the deduction, either the lower bound of ``y`` (l.5) or the upper bound of ``x`` (l.7) explains the deduction.Indeed, such a propagator only updates lower bound of ``y`` based on the upper bound of ``x`` and *vice versa*.Let consider that the deduction involves ``x`` and is explained by the lower bound of ``y``.The lower bound ``y`` needs to be explained.The value below the current lower bound of ``y`` are iterated and each deduction are explained, thanks to the causes previously stored.This is repeated until all deductions are explained.The results is a set of deductions, including branching decisions, and a set a propagators, which applied altogether explained the conflict.Explanations for the system---------------------------Explanations for the system, which try to reduce the search space, differ from the ones giving feedback to a user about the insatisfiability of its model.Both rely on the capacity of the explanation engine to motivate a failure, during the search form system explanations and once the search is complete for user ones... important::    Most of the time, explanations are raw and need to be processed to be easily interpreted by users.Conflict-based backjumping^^^^^^^^^^^^^^^^^^^^^^^^^^When Conflict-based Backjumping (CBJ) is plugged-in, the search is hacked in the following way.On a failure, explanations are retrieved.From all left branch decisions explaining the failure, the last taken, *return decision*, is stored to jump back to it.Decisions from the current one to the return decision (excluded) are erased.Then, the return decision is refuted and the search goes on.If the explanation is made of no left branch decision, the problem is proven to have no solution and search stops.**Factory**: ``solver.explanations.ExplanationFactory``**API**: ::    CBJ.plugin(Solver solver, boolean flattened)Dynamic backtracking^^^^^^^^^^^^^^^^^^^^This strategy, Dynamic backtracking (DBT) corrects a lack of deduction of Conflict-based backjumping.On a failure, explanations are retrieved.From all left branch decisions explaining the failure, the last taken, *return decision*, is stored to jump back to it.Decisions from the current one to the return decision (excluded) are maintained, only the return decision is refuted and the search goes on.If the explanation is made of no left branch decision, the problem is proven to have no solution and search stops.**Factory**: ``solver.explanations.ExplanationFactory``**API**: ::    DBT.plugin(Solver solver, boolean flattened)Explanations for the end-user-----------------------------Explaining the last failure of a complete search without solution provides information about the reasons why a problem has no solution.For the moment, there is no simplified way to get such explanations.Incomplete search leads to incomplete explanations: as far as at least one decision is part of the explanation, there is no guarantee the failure does not come from that decision.On the other hand, when there is no decision, the explanation is complete... _44_monitors_label:Search monitor==============Principle---------A search monitor is an observer of the search loop.It gives user access before and after executing each main step of the search loop:- `initialize`: when the search loop starts,- `initial propagator`: when the initial propagation is run,- `open node`: when a decision is computed,- `down left branch`: on going down in the tree search applying a decision,- `down right branch`: on going down in the tree search refuting a decision,- `up branch`: on going up in the tree search to reconsider a decision,- `solution`: when a solution is got,- `restart search`: when the search is restarted to a previous node, commonly the root node,- `close`: when the search loop ends,- `contradiction`: on a failure,- `interruption`: on the interruption of the search loop.With the accurate search monitor, one can easily interact with the search loop, from pretty printing of a solution to forcing a restart, or many other actions.The interfaces to implement are:- ``IMonitorInitialize``,- ``IMonitorInitPropagation``,- ``IMonitorOpenNode``,- ``IMonitorDownBranch``,- ``IMonitorUpBranch``,- ``IMonitorSolution``,- ``IMonitorRestart``,- ``IMonitorContradiction``,- ``IMonitorInterruption``,- ``IMonitorClose``.Most of them gives the opportunity to do something before and after a step. The other ones are called after a step.For instance, ``NogoodStoreFromRestarts`` monitors restarts.Before a restart is done, the nogoods are extracted from the current decision path;after the restart has been done, the newly created nogoods are added and the nogoods are propagated.Thus, the framework is almost not intrusive. .. literalinclude:: /../../choco-solver/src/main/java/solver/constraints/nary/nogood/NogoodStoreFromRestarts.java   :language: java   :lines: 55,80-82, 85-99   :linenos:.. _45_define_search_label:Defining its own search strategy================================One key component of the resolution is the exploration of the search space induced by the domains and constraints.It happens that built-in search strategies are not enough to tackle the problem.Or one may want to define its own strategy.This can be done in three steps: selecting the variable, selecting the value, then making a decision.The following instructions are based on IntVar, but can be easily adapted to other types of variables.Selecting the variable----------------------An implementation of the ``VariableSelector<V extends Variable>`` interface is needed.A variable selector specifies which variable should be selected at a fix point.It is based specifications (ex: smallest domain, most constrained, etc.).Although it is not required, the selected variable should not be already instantiated to a singleton.This interface forces to define only one method:    ``V getVariable(V[] variables)`` One variable has to be selected from ``variables`` to create a decision on. If no valid variable exists, the method is expected to return ``null``.An implementation of the ``VariableEvaluator<V extends Variable>`` is strongly recommended.It enables breaking ties. It forces to define only one method:    ``double evaluate(V variable)`` An evaluation of the given variable is done wrt the evaluator. The variable with the **smallest** value will then be selected.Here is the code of the ``FirstFail`` variable selector which selects first the variable with the smallest domain. .. literalinclude:: /../../choco-solver/src/main/java/solver/search/strategy/selectors/variables/FirstFail.java   :language: java   :lines: 42-63   :linenos:Selecting the variable----------------------The value to be selected must belong to the variable domain.For ``IntVar`` the interface ``IntValueSelector`` is required.It imposes one method:    ``int selectValue(IntVar var)`` Return the value to constrain ``var`` with.Making a decision-----------------A decision is made of a variable, an decision operator and a value.The decision operator should be selected in ``DecisionOperator`` among:    ``int_eq`` For ``IntVar``, represents an instantiation, :math:`X = 3`. The refutation of the decision will be a value removal.    ``int_neq`` For ``IntVar``, represents a value removal, :math:`X \neq 3`. The refutation of the decision will be an instantiation.    ``int_split`` For ``IntVar``, represents an upper bound modification, :math:`X \leq 3`. The refutation of the decision will be a lower bound modification.    ``int_reverse_split`` For ``IntVar``, represents a lower bound modification, :math:`X \geq 3`. The refutation of the decision will be an upper bound modification.    ``set_force`` For ``SetVar``, represents a kernel addition, :math:`3 \in S`. The refutation of the decision will be an envelop removal.    ``set_remove`` For ``SetVar``, represents an envelop removal, :math:`3 \notin S`. The refutation of the decision will be a kernel addition... attention::    A particular attention should be made while using ``IntVar``s and their type of domain.    Indeed, bounded variables does not support making holes in their domain.    Thus, removing a value which is not a current bound will be missed, and can lead to an infinite loop.One can define its own operator by extending ``DecisionOperator``.    ``void apply(V var, int value, ICause cause)``  Operations to execute when the decision is applied (left branch).  It can throw an ``ContradictionException`` if the application is not possible.    ``void unapply(V var, int value, ICause cause)``  Operations to execute when the decision is refuted (right branch).  It can throw an ``ContradictionException`` if the application is not possible.    ``DecisionOperator opposite()``  Opposite of the decision operator. *Currently useless*.    ``String toString()``  A pretty print of the decision, for logging.Most of the time, extending ``AbstractStrategy`` is not necessary.Using specific strategy dedicated to a type of variable, such as ``IntStrategy`` is enough.The one above has an alternate constructor: ::    public IntStrategy(IntVar[] scope,                       VariableSelector<IntVar> varSelector,                       IntValueSelector valSelector,                       DecisionOperator<IntVar> decOperator) {...}And defining your own strategy is really crucial, start by copying/pasting an existing one.Indeed, decisions are stored in pool managers to avoid creating too many decision objects, and thus garbage collecting too often... _46_define_constraint_label:Defining its own constraint===========================.. important::    The array of variables given in parameter of a ``Propagator`` constructor is not cloned but referenced.    That is, if a permutation occurs in the array of variables, all propagators referencing the array will be incorrect... _47_ibex:Ibex====    "IBEX is a C++ library for constraint processing over real numbers.    It provides reliable algorithms for handling non-linear constraints.    In particular, roundoff errors are also taken into account.    It is based on interval arithmetic and affine arithmetic."    -- http://www.ibex-lib.org/To manage continuous constraints with Choco, an interface with Ibex has been done.It needs Ibex to be installed on your system.Then, simply declare the following VM options:.. code-block:: none    -Djava.library.path=/path/to/Ibex/libThe path `/path/to/Ibex/lib` points to the `lib` directory of the Ibex installation directory.Installing Ibex---------------See the `installation instructions <http://www.ibex-lib.org/doc/install.html>`_ of Ibex to complied Ibex on your system.More specially, take a look at `Installation as a dynamic library <http://www.ibex-lib.org/doc/install.html#installation-as-a-dynamic-library>`_and do not forget to add the ``--with-java-package=solver.constraints.real`` configuration option.Once the installation is completed, the JVM needs to know where Ibex is installed to fully benefit from the Choco-Ibex bridge and declare real variables and constraints.